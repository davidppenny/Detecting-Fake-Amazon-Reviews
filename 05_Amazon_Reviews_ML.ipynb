{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews Analysis\n",
    "---\n",
    "<b>By David Penny<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing our (very) favorite libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical = pd.read_csv('df_numerical.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>falsified</th>\n",
       "      <th>rating</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_words_in_text</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words_in_text_no_stop</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_scentences_in_text</th>\n",
       "      <th>flesch_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>overall_readability_index</th>\n",
       "      <th>total_sentiment</th>\n",
       "      <th>average_review_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>4.086957</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>102.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   falsified  rating  verified_purchase  sentiment  num_words_in_text  \\\n",
       "0          1       4                  0          1                 23   \n",
       "\n",
       "   num_stopwords  num_words_in_text_no_stop  num_unique_words  mean_word_len  \\\n",
       "0             11                         12                21       4.086957   \n",
       "\n",
       "   num_chars  num_punctuations  num_scentences_in_text  flesch_ease  \\\n",
       "0        116                 3                       2        102.1   \n",
       "\n",
       "   flesch_kincaid_grade  automated_readability_index  \\\n",
       "0                   1.9                          3.6   \n",
       "\n",
       "   overall_readability_index  total_sentiment  average_review_sentiment  \n",
       "0                        5.0              6.0                       3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19989, 18)\n"
     ]
    }
   ],
   "source": [
    "display(df_numerical.head(1))\n",
    "print(df_numerical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled = pd.read_csv('df_downsampled.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>falsified</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>overall_readability_index</th>\n",
       "      <th>total_sentiment</th>\n",
       "      <th>average_review_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   falsified  verified_purchase  sentiment  num_punctuations  \\\n",
       "0          1                  1          1                 5   \n",
       "\n",
       "   flesch_kincaid_grade  overall_readability_index  total_sentiment  \\\n",
       "0                  10.5                       11.0             16.0   \n",
       "\n",
       "   average_review_sentiment  \n",
       "0                       8.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9104, 8)\n"
     ]
    }
   ],
   "source": [
    "display(df_downsampled.head(1))\n",
    "print(df_downsampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['falsified', 'verified_purchase', 'sentiment', 'num_punctuations',\n",
       "       'flesch_kincaid_grade', 'overall_readability_index', 'total_sentiment',\n",
       "       'average_review_sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downsampled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we choose which version of the df we will use\n",
    "#dataframe = df_numerical\n",
    "dataframe = df_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's get a baseline to see how a dummy classifier would score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels:  [1]\n",
      "Test score:  0.49615975422427033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate input features and target\n",
    "y = df_final['falsified']\n",
    "X = df_final.drop(columns='falsified', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "# DummyClassifier to predict only target 0\n",
    "dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "# checking unique labels\n",
    "print('Unique predicted labels: ', (np.unique(dummy_pred)))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test score: ', accuracy_score(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by testing a very basic NN model to get another baseline for our accuracy potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9104,)\n",
      "(9104, 7)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c46849225267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mNN_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The test accuracy is: {NN_model.score(X,y):0.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \"\"\"\n\u001b[1;32m   1027\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m-> 1028\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n\u001b[0;32m--> 376\u001b[0;31m                             intercept_grads, layer_units)\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    468\u001b[0m                     \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                 },\n\u001b[0;32m--> 470\u001b[0;31m                 args=(X, y, activations, deltas, coef_grads, intercept_grads))\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_optimize_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[0;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_coef_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         loss, coef_grads, intercept_grads = self._backprop(\n\u001b[0;32m--> 178\u001b[0;31m             X, y, activations, deltas, coef_grads, intercept_grads)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0minplace_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDERIVATIVES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0minplace_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             coef_grads, intercept_grads = self._compute_loss_grad(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/neural_network/_base.py\u001b[0m in \u001b[0;36minplace_relu_derivative\u001b[0;34m(Z, delta)\u001b[0m\n\u001b[1;32m    168\u001b[0m          \u001b[0mThe\u001b[0m \u001b[0mbackpropagated\u001b[0m \u001b[0merror\u001b[0m \u001b[0msignal\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmodified\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \"\"\"\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mZ\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y = dataframe['falsified']\n",
    "X = dataframe.drop(columns='falsified')\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "NN_model = MLPClassifier(hidden_layer_sizes=(128,128,128), solver='lbfgs', max_iter=10000)\n",
    "NN_model.fit(X,y);\n",
    "\n",
    "print(f'The test accuracy is: {NN_model.score(X,y):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5826 train examples\n",
      "1457 validation examples\n",
      "1821 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('falsified')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['verified_purchase', 'sentiment', 'num_punctuations', 'flesch_kincaid_grade', 'overall_readability_index', 'total_sentiment', 'average_review_sentiment']\n",
      "A batch of ages: tf.Tensor([ 9.  4. 16.  9. 13.], shape=(5,), dtype=float64)\n",
      "A batch of targets: tf.Tensor([1 1 0 1 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ages:', feature_batch['total_sentiment'])\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('falsified')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['verified_purchase', 'sentiment', 'num_punctuations', 'flesch_kincaid_grade', 'overall_readability_index', 'total_sentiment', 'average_review_sentiment']\n",
      "A batch of ages: tf.Tensor([14. 15. 12. 10. 22.], shape=(5,), dtype=float64)\n",
      "A batch of targets: tf.Tensor([1 1 0 1 1], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ages:', feature_batch['total_sentiment'])\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A utility method to create a feature column\n",
    "# # and to transform a batch of data\n",
    "# def demo(feature_column):\n",
    "#   feature_layer = layers.DenseFeatures(feature_column)\n",
    "#   print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_sentiment = feature_column.numeric_column(\"total_sentiment\")\n",
    "# demo(total_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_sentiment_buckets = feature_column.bucketized_column(total_sentiment, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "# demo(total_sentiment_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "#       'thal', ['fixed', 'normal', 'reversible'])\n",
    "\n",
    "# thal_one_hot = feature_column.indicator_column(thal)\n",
    "# demo(thal_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notice the input to the embedding column is the categorical column\n",
    "# # we previously created\n",
    "# thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "# demo(thal_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thal_hashed = feature_column.categorical_column_with_hash_bucket(\n",
    "#       'thal', hash_bucket_size=1000)\n",
    "# demo(feature_column.indicator_column(thal_hashed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "# demo(feature_column.indicator_column(crossed_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# # numeric cols\n",
    "# for header in ['rating',\n",
    "#                'verified_purchase',\n",
    "#                'sentiment',\n",
    "#                'num_words_in_text',\n",
    "#                'num_stopwords',\n",
    "#                'num_words_in_text_no_stop', \n",
    "#                'num_unique_words', \n",
    "#                'mean_word_len',\n",
    "#                'num_chars',\n",
    "#                'num_punctuations',\n",
    "#                'num_scentences_in_text', \n",
    "#                'flesch_ease', \n",
    "#                'flesch_kincaid_grade',\n",
    "#                'automated_readability_index',\n",
    "#                'overall_readability_index', \n",
    "#                'total_sentiment',\n",
    "#                'average_review_sentiment']:\n",
    "#   feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# DOWNSAMPLED numeric cols\n",
    "for header in ['verified_purchase', 'sentiment', 'num_punctuations',\n",
    "       'flesch_kincaid_grade', 'overall_readability_index', 'total_sentiment',\n",
    "       'average_review_sentiment']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# # bucketized cols\n",
    "# age_buckets = feature_column.bucketized_column(total_sentiment, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "# feature_columns.append(age_buckets)\n",
    "\n",
    "# # indicator cols\n",
    "# thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "#       'thal', ['fixed', 'normal', 'reversible'])\n",
    "# thal_one_hot = feature_column.indicator_column(thal)\n",
    "# feature_columns.append(thal_one_hot)\n",
    "\n",
    "# # embedding cols\n",
    "# thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "# feature_columns.append(thal_embedding)\n",
    "\n",
    "# # crossed cols\n",
    "# crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "# crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "# feature_columns.append(crossed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.7059 - accuracy: 0.4856 - val_loss: 0.6942 - val_accuracy: 0.5024\n",
      "Epoch 2/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6956 - accuracy: 0.4818 - val_loss: 0.6958 - val_accuracy: 0.5024\n",
      "Epoch 3/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.4818 - val_loss: 0.6883 - val_accuracy: 0.5024\n",
      "Epoch 4/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6793 - accuracy: 0.4940 - val_loss: 0.6505 - val_accuracy: 0.5051\n",
      "Epoch 5/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6226 - accuracy: 0.6274 - val_loss: 0.5954 - val_accuracy: 0.7076\n",
      "Epoch 6/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.5584 - accuracy: 0.7096 - val_loss: 0.5277 - val_accuracy: 0.7632\n",
      "Epoch 7/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.5005 - accuracy: 0.7703 - val_loss: 0.4837 - val_accuracy: 0.7852\n",
      "Epoch 8/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4776 - accuracy: 0.7949 - val_loss: 0.4993 - val_accuracy: 0.7742\n",
      "Epoch 9/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4674 - accuracy: 0.7973 - val_loss: 0.4717 - val_accuracy: 0.7962\n",
      "Epoch 10/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4597 - accuracy: 0.8031 - val_loss: 0.4791 - val_accuracy: 0.7968\n",
      "Epoch 11/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.8067 - val_loss: 0.4692 - val_accuracy: 0.7982\n",
      "Epoch 12/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4530 - accuracy: 0.8088 - val_loss: 0.4727 - val_accuracy: 0.8099\n",
      "Epoch 13/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4518 - accuracy: 0.8115 - val_loss: 0.4683 - val_accuracy: 0.7996\n",
      "Epoch 14/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4496 - accuracy: 0.8079 - val_loss: 0.4641 - val_accuracy: 0.8071\n",
      "Epoch 15/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4490 - accuracy: 0.8096 - val_loss: 0.4747 - val_accuracy: 0.8106\n",
      "Epoch 16/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4474 - accuracy: 0.8119 - val_loss: 0.4628 - val_accuracy: 0.8078\n",
      "Epoch 17/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4476 - accuracy: 0.8105 - val_loss: 0.4675 - val_accuracy: 0.8113\n",
      "Epoch 18/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4470 - accuracy: 0.8129 - val_loss: 0.4672 - val_accuracy: 0.8113\n",
      "Epoch 19/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4440 - accuracy: 0.8134 - val_loss: 0.4617 - val_accuracy: 0.8099\n",
      "Epoch 20/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4454 - accuracy: 0.8136 - val_loss: 0.4592 - val_accuracy: 0.8078\n",
      "Epoch 21/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4444 - accuracy: 0.8124 - val_loss: 0.4683 - val_accuracy: 0.8119\n",
      "Epoch 22/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.8138 - val_loss: 0.4610 - val_accuracy: 0.8051\n",
      "Epoch 23/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4436 - accuracy: 0.8143 - val_loss: 0.4579 - val_accuracy: 0.8099\n",
      "Epoch 24/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.8145 - val_loss: 0.4610 - val_accuracy: 0.8133\n",
      "Epoch 25/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.8143 - val_loss: 0.4705 - val_accuracy: 0.8119\n",
      "Epoch 26/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.8141 - val_loss: 0.4666 - val_accuracy: 0.8140\n",
      "Epoch 27/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4424 - accuracy: 0.8143 - val_loss: 0.4662 - val_accuracy: 0.8126\n",
      "Epoch 28/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4412 - accuracy: 0.8145 - val_loss: 0.4576 - val_accuracy: 0.8140\n",
      "Epoch 29/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4432 - accuracy: 0.8134 - val_loss: 0.4607 - val_accuracy: 0.8126\n",
      "Epoch 30/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4407 - accuracy: 0.8158 - val_loss: 0.4746 - val_accuracy: 0.8133\n",
      "Epoch 31/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4406 - accuracy: 0.8146 - val_loss: 0.4563 - val_accuracy: 0.8085\n",
      "Epoch 32/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.8139 - val_loss: 0.4624 - val_accuracy: 0.8133\n",
      "Epoch 33/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.8153 - val_loss: 0.4565 - val_accuracy: 0.8099\n",
      "Epoch 34/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4432 - accuracy: 0.8145 - val_loss: 0.4551 - val_accuracy: 0.8085\n",
      "Epoch 35/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4383 - accuracy: 0.8150 - val_loss: 0.4586 - val_accuracy: 0.8140\n",
      "Epoch 36/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8155 - val_loss: 0.4559 - val_accuracy: 0.8147\n",
      "Epoch 37/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4395 - accuracy: 0.8153 - val_loss: 0.4568 - val_accuracy: 0.8154\n",
      "Epoch 38/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.8127 - val_loss: 0.4683 - val_accuracy: 0.8126\n",
      "Epoch 39/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4403 - accuracy: 0.8162 - val_loss: 0.4535 - val_accuracy: 0.8113\n",
      "Epoch 40/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4376 - accuracy: 0.8175 - val_loss: 0.4550 - val_accuracy: 0.8085\n",
      "Epoch 41/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4388 - accuracy: 0.8143 - val_loss: 0.4536 - val_accuracy: 0.8133\n",
      "Epoch 42/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.8157 - val_loss: 0.4707 - val_accuracy: 0.8126\n",
      "Epoch 43/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8177 - val_loss: 0.4553 - val_accuracy: 0.8154\n",
      "Epoch 44/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.8181 - val_loss: 0.4530 - val_accuracy: 0.8140\n",
      "Epoch 45/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.8162 - val_loss: 0.4623 - val_accuracy: 0.8161\n",
      "Epoch 46/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4380 - accuracy: 0.8172 - val_loss: 0.4553 - val_accuracy: 0.8147\n",
      "Epoch 47/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.8181 - val_loss: 0.4570 - val_accuracy: 0.8051\n",
      "Epoch 48/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.8163 - val_loss: 0.4527 - val_accuracy: 0.8119\n",
      "Epoch 49/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8169 - val_loss: 0.4572 - val_accuracy: 0.8147\n",
      "Epoch 50/50\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8170 - val_loss: 0.4542 - val_accuracy: 0.8106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14da080b8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(4, activation='sigmoid'),\n",
    "  layers.Dense(128, activation='sigmoid'),\n",
    "  layers.Dense(128, activation='sigmoid'),\n",
    "  layers.Dense(128, activation='sigmoid'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.8215\n",
      "Accuracy 0.8215266466140747\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps\n",
    "The best way to learn more about classifying structured data is to try it yourself. We suggest finding another dataset to work with, and training a model to classify it using code similar to the above. To improve accuracy, think carefully about which features to include in your model, and how they should be represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/bin/tensorboard\", line 8, in <module>\n",
       "    sys.exit(run_main())\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/main.py\", line 65, in run_main\n",
       "    default.get_plugins() + default.get_dynamic_plugins(),\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/default.py\", line 125, in get_dynamic_plugins\n",
       "    \"tensorboard_plugins\"\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/default.py\", line 124, in <listcomp>\n",
       "    for entry_point in pkg_resources.iter_entry_points(\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2460, in load\n",
       "    self.require(*args, **kwargs)\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2483, in require\n",
       "    items = working_set.resolve(reqs, env, installer, extras=self.extras)\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 791, in resolve\n",
       "    raise VersionConflict(dist, req).with_context(dependent_req)\n",
       "pkg_resources.VersionConflict: (grpcio 1.23.0 (/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages), Requirement.parse('grpcio>=1.24.3'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
