{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews Analysis\n",
    "---\n",
    "<b>By David Penny<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing our (very) favorite libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical = pd.read_csv('df_numerical.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>falsified</th>\n",
       "      <th>rating</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_words_in_text</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words_in_text_no_stop</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_scentences_in_text</th>\n",
       "      <th>flesch_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>overall_readability_index</th>\n",
       "      <th>total_sentiment</th>\n",
       "      <th>average_review_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>4.086957</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>102.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   falsified  rating  verified_purchase  sentiment  num_words_in_text  \\\n",
       "0          1       4                  0          1                 23   \n",
       "\n",
       "   num_stopwords  num_words_in_text_no_stop  num_unique_words  mean_word_len  \\\n",
       "0             11                         12                21       4.086957   \n",
       "\n",
       "   num_chars  num_punctuations  num_scentences_in_text  flesch_ease  \\\n",
       "0        116                 3                       2        102.1   \n",
       "\n",
       "   flesch_kincaid_grade  automated_readability_index  \\\n",
       "0                   1.9                          3.6   \n",
       "\n",
       "   overall_readability_index  total_sentiment  average_review_sentiment  \n",
       "0                        5.0              6.0                       3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19989, 18)\n"
     ]
    }
   ],
   "source": [
    "display(df_numerical.head(1))\n",
    "print(df_numerical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled = pd.read_csv('df_downsampled.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>falsified</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>overall_readability_index</th>\n",
       "      <th>total_sentiment</th>\n",
       "      <th>average_review_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   falsified  verified_purchase  sentiment  num_punctuations  \\\n",
       "0          1                  1          1                 5   \n",
       "\n",
       "   flesch_kincaid_grade  overall_readability_index  total_sentiment  \\\n",
       "0                  10.5                       11.0             16.0   \n",
       "\n",
       "   average_review_sentiment  \n",
       "0                       8.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9104, 8)\n"
     ]
    }
   ],
   "source": [
    "display(df_downsampled.head(1))\n",
    "print(df_downsampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['falsified', 'verified_purchase', 'sentiment', 'num_punctuations',\n",
       "       'flesch_kincaid_grade', 'overall_readability_index', 'total_sentiment',\n",
       "       'average_review_sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downsampled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we choose which version of the df we will use\n",
    "#dataframe = df_numerical\n",
    "dataframe = df_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's get a baseline to see how a dummy classifier would score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels:  [1]\n",
      "Test score:  0.49615975422427033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate input features and target\n",
    "y = df_final['falsified']\n",
    "X = df_final.drop(columns='falsified', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "# DummyClassifier to predict only target 0\n",
    "dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "# checking unique labels\n",
    "print('Unique predicted labels: ', (np.unique(dummy_pred)))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test score: ', accuracy_score(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20830,)\n",
      "(20830, 23)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Sports'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-331-7123fba7a7db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    696\u001b[0m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[1;32m    697\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                                 force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 )\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Sports'"
     ]
    }
   ],
   "source": [
    "y = df_final['falsified']\n",
    "X = df_final.drop(columns='falsified')\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "NN_model = MLPClassifier(hidden_layer_sizes=(128,128,128), solver='lbfgs', max_iter=10000)\n",
    "NN_model.fit(X,y);\n",
    "\n",
    "print(f'The test accuracy is: {NN_model.score(X,y):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5826 train examples\n",
      "1457 validation examples\n",
      "1821 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('falsified')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['verified_purchase', 'sentiment', 'num_punctuations', 'flesch_kincaid_grade', 'overall_readability_index', 'total_sentiment', 'average_review_sentiment']\n",
      "A batch of ages: tf.Tensor([12. 18. 15.  9.  4.], shape=(5,), dtype=float64)\n",
      "A batch of targets: tf.Tensor([0 1 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ages:', feature_batch['total_sentiment'])\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('falsified')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['verified_purchase', 'sentiment', 'num_punctuations', 'flesch_kincaid_grade', 'overall_readability_index', 'total_sentiment', 'average_review_sentiment']\n",
      "A batch of ages: tf.Tensor([12. 11. 10.  8.  7.], shape=(5,), dtype=float64)\n",
      "A batch of targets: tf.Tensor([1 1 0 0 1], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ages:', feature_batch['total_sentiment'])\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A utility method to create a feature column\n",
    "# # and to transform a batch of data\n",
    "# def demo(feature_column):\n",
    "#   feature_layer = layers.DenseFeatures(feature_column)\n",
    "#   print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_sentiment = feature_column.numeric_column(\"total_sentiment\")\n",
    "# demo(total_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_sentiment_buckets = feature_column.bucketized_column(total_sentiment, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "# demo(total_sentiment_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "#       'thal', ['fixed', 'normal', 'reversible'])\n",
    "\n",
    "# thal_one_hot = feature_column.indicator_column(thal)\n",
    "# demo(thal_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notice the input to the embedding column is the categorical column\n",
    "# # we previously created\n",
    "# thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "# demo(thal_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thal_hashed = feature_column.categorical_column_with_hash_bucket(\n",
    "#       'thal', hash_bucket_size=1000)\n",
    "# demo(feature_column.indicator_column(thal_hashed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "# demo(feature_column.indicator_column(crossed_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# # numeric cols\n",
    "# for header in ['rating',\n",
    "#                'verified_purchase',\n",
    "#                'sentiment',\n",
    "#                'num_words_in_text',\n",
    "#                'num_stopwords',\n",
    "#                'num_words_in_text_no_stop', \n",
    "#                'num_unique_words', \n",
    "#                'mean_word_len',\n",
    "#                'num_chars',\n",
    "#                'num_punctuations',\n",
    "#                'num_scentences_in_text', \n",
    "#                'flesch_ease', \n",
    "#                'flesch_kincaid_grade',\n",
    "#                'automated_readability_index',\n",
    "#                'overall_readability_index', \n",
    "#                'total_sentiment',\n",
    "#                'average_review_sentiment']:\n",
    "#   feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# DOWNSAMPLED numeric cols\n",
    "for header in ['verified_purchase', 'sentiment', 'num_punctuations',\n",
    "       'flesch_kincaid_grade', 'overall_readability_index', 'total_sentiment',\n",
    "       'average_review_sentiment']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# # bucketized cols\n",
    "# age_buckets = feature_column.bucketized_column(total_sentiment, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "# feature_columns.append(age_buckets)\n",
    "\n",
    "# # indicator cols\n",
    "# thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "#       'thal', ['fixed', 'normal', 'reversible'])\n",
    "# thal_one_hot = feature_column.indicator_column(thal)\n",
    "# feature_columns.append(thal_one_hot)\n",
    "\n",
    "# # embedding cols\n",
    "# thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "# feature_columns.append(thal_embedding)\n",
    "\n",
    "# # crossed cols\n",
    "# crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "# crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "# feature_columns.append(crossed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_features_26 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.6969 - accuracy: 0.4882 - val_loss: 0.6918 - val_accuracy: 0.4804\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6941 - accuracy: 0.4882 - val_loss: 0.6914 - val_accuracy: 0.4804\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.4900 - val_loss: 0.6888 - val_accuracy: 0.4804\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6557 - accuracy: 0.5113 - val_loss: 0.6268 - val_accuracy: 0.5937\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.6138 - accuracy: 0.6608 - val_loss: 0.5784 - val_accuracy: 0.6932\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.5511 - accuracy: 0.7455 - val_loss: 0.5166 - val_accuracy: 0.7756\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7812 - val_loss: 0.4950 - val_accuracy: 0.7694\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.4862 - accuracy: 0.7856 - val_loss: 0.4747 - val_accuracy: 0.7955\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4712 - accuracy: 0.7903 - val_loss: 0.4681 - val_accuracy: 0.7914\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.7956 - val_loss: 0.4636 - val_accuracy: 0.8003\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4634 - accuracy: 0.7973 - val_loss: 0.4661 - val_accuracy: 0.8058\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.7993 - val_loss: 0.4612 - val_accuracy: 0.8071\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4599 - accuracy: 0.7999 - val_loss: 0.4812 - val_accuracy: 0.8099\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4549 - accuracy: 0.8028 - val_loss: 0.4580 - val_accuracy: 0.7968\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4573 - accuracy: 0.8005 - val_loss: 0.4555 - val_accuracy: 0.7996\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.8031 - val_loss: 0.4526 - val_accuracy: 0.8051\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4487 - accuracy: 0.8035 - val_loss: 0.4567 - val_accuracy: 0.8126\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.8055 - val_loss: 0.4559 - val_accuracy: 0.8010\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4489 - accuracy: 0.8090 - val_loss: 0.4493 - val_accuracy: 0.8119\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.4489 - accuracy: 0.8086 - val_loss: 0.4487 - val_accuracy: 0.8133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1482787b8>"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(4, activation='sigmoid'),\n",
    "  layers.Dense(128, activation='sigmoid'),\n",
    "  layers.Dense(128, activation='sigmoid'),\n",
    "  layers.Dense(128, activation='sigmoid'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7895\n",
      "Accuracy 0.789486289024353\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps\n",
    "The best way to learn more about classifying structured data is to try it yourself. We suggest finding another dataset to work with, and training a model to classify it using code similar to the above. To improve accuracy, think carefully about which features to include in your model, and how they should be represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/bin/tensorboard\", line 8, in <module>\n",
       "    sys.exit(run_main())\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/main.py\", line 65, in run_main\n",
       "    default.get_plugins() + default.get_dynamic_plugins(),\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/default.py\", line 125, in get_dynamic_plugins\n",
       "    \"tensorboard_plugins\"\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/default.py\", line 124, in <listcomp>\n",
       "    for entry_point in pkg_resources.iter_entry_points(\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2460, in load\n",
       "    self.require(*args, **kwargs)\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2483, in require\n",
       "    items = working_set.resolve(reqs, env, installer, extras=self.extras)\n",
       "  File \"/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 791, in resolve\n",
       "    raise VersionConflict(dist, req).with_context(dependent_req)\n",
       "pkg_resources.VersionConflict: (grpcio 1.23.0 (/Users/davepenny/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages), Requirement.parse('grpcio>=1.24.3'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
